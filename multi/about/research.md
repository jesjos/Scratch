----- 
fr: title: Recherche
en: title: Research
menupriority: 1
-----

<% 
def dees
    %{<a href="#{murl(dees)}"><span class="sc">dees</span></a>}
end
def sedil
    %{<a href="#{murl(sedil)}"><span class="sc">sed</span>i<span class="sc">l</span></a>}
end
%>

## Abstract

My research area is statistical machine learning for sequence modeling.
In particular, I was interrested in subclasses 
of multiplicity automata (also called weighted finite state machines
or weighted finite state automata):

- Markov chains
- Deterministic probabilistic automata
- Hidden Markov Models (<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">HMM</a>), probabilistic automata
- Bayesian Network
- Multiplicity automata

Recently, I'm working on generalisations of multiplicity automata:
      
- Weighted Tree Automata
- Weighted Transducer
- Weighted Finite State Machine with n tapes

I'm also working on using multiplicity automata for image, sound
and temporal sequence recognition.
In particular, I work with edit distances in order to provide a 
good mesure between structured representations.

## Algorithms

### <%= sedil %>

I developed a software platform which learn edit distances betweens trees or
between sequences from sequences or forest. 
This platform <%= sedil %>
was created in JAVA (1.5) using Swing for the user interface.
This work was done during my post-PhD with Marc Sebban for the Marmota
Project.

### <%= dees %>
  
I developed an inference algorithm of multiplicity automata from sequence.
This program called <%= dees %> is implemented in C++.
We proved with
<a href="http://www.lif.univ-mrs.fr/~fdenis">Fran√ßois Denis</a> and
<a href="http://www.lif.univ-mrs.fr/~habrard">Amaury Habrard</a> 
this algorithm has many interesting theoretical properties
(see <a href="/Scratch/files/colt2006.pdf">[COLT'06]</a> or 
 <a href="/Scratch/files/cap2006.pdf">[CAp'06]</a> for more details):

- small size of models (a good point in machine learning)
- Any stochastic rational serie (in particular HMM) can be identified in the limit with probability one with this algorithm

These positives learning results are surprising since the identified
class is not recursively enumerable!

In practice, the algorithm has a good behaviour as expected by
theoretical results (see <a href="/Scratch/files/icgi2006.pdf">[ICGI'06]</a>). 


## Main Theoretical Results

For sake of clarity I will denote
<em>Multiplicity Automaton</em> (resp.  
        <em>Probabilistic Automaton</em>, 
        <em>Probabilistic Deterministic Automaton</em>) by 
<em>MA</em> (resp. <em>PA</em>, <em>PDA</em>).

### Machine Learning

Elaboration of the algorithm <%= dees %> which identify in the limit with probability 1 the class of stochastic rational series.

When <%= dees %> is constrained to generate PA it returns only PRA.
The class of probabilistic distribution generated by PRA is identified in the limit with probability 1.

<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">HMM</a> 
and PA
are identifiable in the limit with probability 1, but
the proof use an algorithm unusable for real uses.

### Language Theory 

In <em>Probabilistic Automata</em> written by Azaria Paz in 1977,
some problems was left open:

Given an MA with real parameters (positives and negative): 

- is it decidable to know whether it generates a positive rational series?
- is it decidable to know whether it generates a probabilisitc distribution?

- Is the set of rational series which are probabilistic distribution equal to the set of which that are generated only by positive parameter MA?
     
The answer is negative for these three questions. 


Introduction of a new intermediary class between PA and PDA ;
<em>Probabilistic Residual Automata</em> (<b><em>PRA</em></b>).
This class has many interresting properties.

Each automata class (MA, PA, PRA and PDA) generates different probabilistic
distributions class.

The class of MA generating probabilistic distribution
is not recursively enumerable.

Studies of reduction and saturation operator for
each class ; MA, PA, PRA and PDA.

<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">
(Hidden Markov Model)
</a> are equivalent to PA. 

## Applications 

This work is particularly well suited for domain where data are
sequences. In particular:

- Bio-informatic (nucleotide or amino acid sequences)
- Video (sequences of images)
- Handwriting recognition (temporal position sequences)
- Natural language (word sequences)
